**Comporative analysis of capability Spark 3.2.1 vs 3.3.0**

<br>In current analyses used 3 warning levels *'Warn level'*:
1. *not_critical* - changes witch are extanding functionality or alow to redifine some of function parameters;
2. *critical* - changes witch are need to manual redefine or refactor parameters;
3. *allowed* - changes do not affect to the current one, becouse default values have a ```'false'``` args.

<br>Changes with *allowed* type of warning are nit include in the current analilsys. 
<br>
| Function                                                                                                 	| Describe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                	| Warn level    	| Brief conclusion                                                                                           	|
|----------------------------------------------------------------------------------------------------------	|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|---------------	|------------------------------------------------------------------------------------------------------------	|
| log4j2 spark.yarn.am.clientModeTreatDisconnectAsFailed                                                   	| Treat yarn-client unclean disconnects as failures. In yarn-client mode, normally the application will always finish with a final status of SUCCESS because in some cases, it is not possible to know if the Application was terminated intentionally by the user or if there was a real error. This config changes that behavior such that if the Application Master disconnects from the driver uncleanly (ie without the proper shutdown handshake) the application will terminate with a final status of FAILED. This will allow the caller to decide if it was truly a failure. Note that if this config is set and the user just terminate the client application badly it may show a status of FAILED when it wasn't really FAILED.                               	| not_critical  	| not critical until log4gj1, used in version 3.2.1, becomes legacy                                          	|
| log4j2 spark.yarn.am.clientModeExitOnError                                                               	|  In yarn-client mode, when this is true, if driver got application report with final status of KILLED or FAILED, driver will stop corresponding SparkContext and exit program with code 1. Note, if this is true and called from another application, it will terminate the parent application as well.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 	| not_critical  	| not critical until log4gj1, used in version 3.2.1, becomes legacy                                          	|
| log4j2 spark.yarn.shuffle.service.logs.namespace                                                         	| A namespace which will be appended to the class name when forming the logger name to use for emitting logs from the YARN shuffle service, like org.apache.spark.network.yarn.YarnShuffleService.logsNamespaceValue. Since some logging frameworks may expect the logger name to look like a class name, it's generally recommended to provide a value which would be a valid Java package or class name and not include spaces.                                                                                                                                                                                                                                                                                                                                         	| not_critical  	| not critical until log4gj1, used in version 3.2.1, becomes legacy                                          	|
| spark.kubernetes.memoryOverheadFactor                                                                    	| This sets the Memory Overhead Factor that will allocate memory to non-JVM memory, which includes off-heap memory allocations, non-JVM tasks, various systems processes, and tmpfs-based local directories when spark.kubernetes.local.dirs.tmpfs is true. For JVM-based jobs this value will default to 0.10 and 0.40 for non-JVM jobs. This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly fail with "Memory Overhead Exceeded" errors. This preempts this error with a higher default.  This will be overridden by the value set by spark.driver.memoryOverheadFactor and spark.executor.memoryOverheadFactor explicitly.                                                                                                              	| not_critical  	| extending functionality                                                                                    	|
| Self-Contained Applications                                                                              	| SCALA ```libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.3.0"```                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          	| critical      	| need to refactor version arg                                                                               	|
| Rate Per Micro-Batch Source (format: rate-micro-batch)                                                   	| rowsPerBatch (e.g. 100): How many rows should be generated per micro-batch.  numPartitions (e.g. 10, default: Spark's default parallelism): The partition number for the generated rows.  startTimestamp (e.g. 1000, default: 0): starting value of generated time.  advanceMillisPerBatch (e.g. 1000, default: 1000): the amount of time being advanced in generated time on each micro-batch.                                                                                                                                                                                                                                                                                                                                                                         	| not_critical  	| extending functionality                                                                                    	|
| Types of time windows  session_window                                                                    	| ```.groupBy(         Column(sessionWindow),         $"userId")```  By default, Spark does not perform partial aggregation for session window aggregation, since it requires additional sort in local partitions before grouping. It works better for the case there are only few number of input rows in same group key for each local partition, but for the case there are numerous input rows having same group key in local partition, doing partial aggregation can still increase the performance significantly despite additional sort.  You can enable spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition to indicate Spark to perform partial aggregation.                                                                                    	| not_critical  	| extending functionality                                                                                    	|
| spark.sql.streaming.stateStore.rocksdb.trackTotalNumberOfRows                                            	| Whether we track the total number of rows in state store. Please refer the details in Performance-aspect considerations.  You may want to disable the track of total number of rows to aim the better performance on RocksDB state store                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                	| not_critical  	| extending functionality                                                                                    	|
| SparkR                                                                                                   	| ```sparkR.session(sparkPackages = "org.apache.spark:spark-avro_2.12:3.3.0")```                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                	| critical      	| need to refactor version arg                                                                               	|
| spark.driver.memoryOverhead  driverMemory * spark.driver.memoryOverheadFactor, with minimum of 384       	| Amount of non-heap memory to be allocated per driver process in cluster mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size (typically 6-10%). This option is currently supported on YARN, Mesos and Kubernetes. Note: Non-heap memory includes off-heap memory (when spark.memory.offHeap.enabled=true) and memory used by other driver processes (e.g. python process that goes with a PySpark driver) and memory used by other non-driver processes running in the same container. The maximum memory size of container to running driver is determined by the sum of spark.driver.memoryOverhead and spark.driver.memory. 	| not_critical  	| extending functionality, allow to redifine memory overhead arg                                             	|
| spark.driver.memoryOverheadFactor 0.10                                                                   	|  Fraction of driver memory to be allocated as additional non-heap memory per driver process in cluster mode. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size. This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to 0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly fail with "Memory Overhead Exceeded" errors. This preempts this error with a higher default. This value is ignored if spark.driver.memoryOverhead is set directly.                                                                                                                                                        	| not_critical  	| extending functionality, allow to redifine memory overhead arg                                             	|
| spark.executor.memoryOverhead  executorMemory * spark.executor.memoryOverheadFactor, with minimum of 384 	| Amount of additional memory to be allocated per executor process, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes. Note: Additional memory includes PySpark executor memory (when spark.executor.pyspark.memory is not configured) and memory used by other non-executor processes running in the same container. The maximum memory size of container to running executor is determined by the sum of spark.executor.memoryOverhead, spark.executor.memory, spark.memory.offHeap.size and spark.executor.pyspark.memory.                           	| not_critical  	| extending functionality, allow to redifine memory overhead arg                                             	|
| spark.executor.memoryOverheadFactor  0.10                                                                	| Fraction of executor memory to be allocated as additional non-heap memory per executor process. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size. This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to 0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly fail with "Memory Overhead Exceeded" errors. This preempts this error with a higher default. This value is ignored if spark.executor.memoryOverhead is set directly.                                                                                                                                                                   	| not_critical  	| extending functionality, allow to redifine memory overhead arg                                             	|
| spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor  0.2                                          	| A partition will be merged during splitting if its size is small than this factor multiply spark.sql.adaptive.advisoryPartitionSizeInBytes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             	| not_critical  	| extending functionality, allow to redifine multiplity factor (prtitiong size)                              	|
| spark.sql.hive.convertMetastoreInsertDir  true                                                           	| When set to true, Spark will try to use built-in data source writer instead of Hive serde in INSERT OVERWRITE DIRECTORY. This flag is effective only if spark.sql.hive.convertMetastoreParquet or spark.sql.hive.convertMetastoreOrc is enabled respectively for Parquet and ORC formats                                                                                                                                                                                                                                                                                                                                                                                                                                                                                	| critical      	| need to analyze the architecture of the project                                                            	|
| spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold  10GB                           	| Byte size threshold of the Bloom filter application side plan's aggregated scan size. Aggregated scan byte size of the Bloom filter application side needs to be over this value to inject a bloom filter.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              	| not_critical  	| extending functionality, defines the conditions for using bloom filter                                     	|
| spark.sql.optimizer.runtime.bloomFilter.creationSideThreshold  10MB                                      	| Size threshold of the bloom filter creation side plan. Estimated size needs to be under this value to try to inject bloom filter.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       	| not_critical  	| extending functionality, defines the conditions for using bloom filter                                     	|
| spark.sql.optimizer.runtime.bloomFilter.expectedNumItems  1000000                                        	| The default number of expected items for the runtime bloomfilter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        	| not_critical  	| extending functionality, defines the conditions for using bloom filter                                     	|
| spark.sql.optimizer.runtime.bloomFilter.maxNumBits  67108864                                             	|  The max number of bits to use for the runtime bloom filter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             	| not_critical  	| extending functionality, defines the conditions for using bloom filter                                     	|
| spark.sql.optimizer.runtime.bloomFilter.maxNumItems  4000000                                             	| The max allowed number of expected items for the runtime bloom filter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   	| not_critical  	| extending functionality, defines the conditions for using bloom filter                                     	|
| spark.sql.optimizer.runtime.bloomFilter.numBits  8388608                                                 	| The default number of bits to use for the runtime bloom filter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          	| not_critical  	| extending functionality, defines the conditions for using bloom filter                                     	|
| spark.sql.optimizer.runtimeFilter.number.threshold  10                                                   	| The total number of injected runtime filters (non-DPP) for a single query. This is to prevent driver OOMs with too many Bloom filters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  	| not_critical  	| extending functionality, defines the conditions for using bloom filter                                     	|
| spark.sql.shuffledHashJoinFactor  3                                                                      	| The shuffle hash join can be selected if the data size of small side multiplied by this factor is still smaller than the large side.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    	| not_critical  	| extending functionality, defines the conditions for the shuffle hash join                                  	|
| spark.sql.sources.disabledJdbcConnProviderList  does not have any default args                           	| Configures a list of JDBC connection providers, which are disabled. The list contains the name of the JDBC connection providers separated by comma.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     	| critical      	| function does not have any default arguments, probably it will need to define it manually or does refactor 	|
| spark.shuffle.push.minShuffleSizeToWait  500m                                                            	| Driver will wait for merge finalization to complete only if total shuffle data size is more than this threshold. If total shuffle size is less, driver will immediately finalize the shuffle output.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    	| not_critical  	| defines the conditions for shuffle merge  operations                                                       	|
| spark.shuffle.push.minCompletedPushRatio  1.0                                                            	| Fraction of minimum map partitions that should be push complete before driver starts shuffle merge finalization during push based shuffle.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              	| not_critical  	| defines the conditions for shuffle merge  operations                                                       	|